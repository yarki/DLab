{"paragraphs":[{"title":"Init Spark","text":"%pyspark\nfrom pyspark.sql import SQLContext\nfrom pyspark.sql import DataFrame\nfrom pyspark.sql import Row\nfrom pyspark.sql.types import *\nimport pandas as pd\nimport StringIO\nimport matplotlib.pyplot as plt\nhc = sc._jsc.hadoopConfiguration()\nhc.set(\"hive.execution.engine\", \"mr\")","dateUpdated":"2017-01-23T08:55:34+0000","config":{"colWidth":6,"editorMode":"ace/mode/scala","title":true,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1485161264995_-611457458","id":"20170116-185159_818793480","dateCreated":"2017-01-23T08:47:44+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:1175","focus":true},{"title":"Define functions","text":"%pyspark\r\nimport csv\r\n\r\nBUCKET = \"jan20fri-dmytro-liaskovskyi-bucket\"\r\n\r\ndef parseCsv(csvStr):\r\n    f = StringIO.StringIO(csvStr)\r\n    reader = csv.reader(f, delimiter=',')\r\n    row = reader.next()\r\n    return row\r\n\r\ndef full_path(part_path):\r\n    return \"s3a://{}/{}\".format(BUCKET, part_path)\r\n\r\nscsv = '\"02Q\",\"Titan Airways\"'\r\nrow = parseCsv(scsv)\r\nprint row[0]\r\nprint row[1]","dateUpdated":"2017-01-23T08:55:34+0000","config":{"colWidth":6,"editorMode":"ace/mode/scala","title":true,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1485161264996_-613381203","id":"20170116-193003_477574066","dateCreated":"2017-01-23T08:47:44+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:1176","focus":true},{"title":"Parse and convert Carrier data to parquet","text":"%pyspark\r\n\r\ncarriersHeader = 'Code,Description'\r\ncarriersText = sc.textFile(full_path(\"carriers.csv\")).filter(lambda x: x != carriersHeader)\r\ncarriers = carriersText.map(lambda s: parseCsv(s)) \\\r\n    .map(lambda s: Row(code=s[0], description=s[1])).cache().toDF()\r\ncarriers.write.mode(\"overwrite\").parquet(full_path(\"processed_local/carriers\"))    \r\nsqlContext.registerDataFrameAsTable(carriers, \"carriers\")\r\ncarriers.limit(20).toPandas()","dateUpdated":"2017-01-23T08:55:34+0000","config":{"colWidth":12,"editorMode":"ace/mode/scala","title":true,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1485161264997_-613765952","id":"20170116-193845_1563104751","dateCreated":"2017-01-23T08:47:44+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:1177","focus":true},{"title":"Parse and convert to parquet Airport data","text":"%pyspark\n\nairportsHeader= '\"iata\",\"airport\",\"city\",\"state\",\"country\",\"lat\",\"long\"'\nairports = sc.textFile(full_path(\"airports.csv\")) \\\n    .filter(lambda x: x != airportsHeader) \\\n    .map(lambda s: parseCsv(s)) \\\n    .map(lambda p: Row(iata=p[0], \\\n                       airport=p[1], \\\n                       city=p[2], \\\n                       state=p[3], \\\n                       country=p[4], \\\n                       lat=float(p[5]), \\\n                       longt=float(p[6])) \\\n        ).cache().toDF()\nairports.write.mode(\"overwrite\").parquet(full_path(\"processed_local/airports\"))    \nsqlContext.registerDataFrameAsTable(airports, \"airports\")\nairports.limit(20).toPandas()","dateUpdated":"2017-01-23T08:55:34+0000","config":{"colWidth":12,"editorMode":"ace/mode/scala","title":true,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1485161264997_-613765952","id":"20170116-194608_52076348","dateCreated":"2017-01-23T08:47:44+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:1178","focus":true},{"title":"Parse and convert Flights data to parquet","text":"%pyspark\n\nflightsHeader = 'Year,Month,DayofMonth,DayOfWeek,DepTime,CRSDepTime,ArrTime,CRSArrTime,UniqueCarrier,FlightNum,TailNum,ActualElapsedTime,CRSElapsedTime,AirTime,ArrDelay,DepDelay,Origin,Dest,Distance,TaxiIn,TaxiOut,Cancelled,CancellationCode,Diverted,CarrierDelay,WeatherDelay,NASDelay,SecurityDelay,LateAircraftDelay'\nflights = sc.textFile(full_path(\"2008.csv.bz2\")) \\\n    .filter(lambda x: x!= flightsHeader) \\\n    .map(lambda s: parseCsv(s)) \\\n    .map(lambda p: Row(Year=int(p[0]), \\\n                       Month=int(p[1]), \\\n                       DayofMonth=int(p[2]), \\\n                       DayOfWeek=int(p[3]), \\\n                       DepTime=p[4], \\\n                       CRSDepTime=p[5], \\\n                       ArrTime=p[6], \\\n                       CRSArrTime=p[7], \\\n                       UniqueCarrier=p[8], \\\n                       FlightNum=p[9], \\\n                       TailNum=p[10], \\\n                       ActualElapsedTime=p[11], \\\n                       CRSElapsedTime=p[12], \\\n                       AirTime=p[13], \\\n                       ArrDelay=int(p[14].replace(\"NA\", \"0\")), \\\n                       DepDelay=int(p[15].replace(\"NA\", \"0\")), \\\n                       Origin=p[16], \\\n                       Dest=p[17], \\\n                       Distance=long(p[18]), \\\n                       TaxiIn=p[19], \\\n                       TaxiOut=p[20], \\\n                       Cancelled=p[21], \\\n                       CancellationCode=p[22], \\\n                       Diverted=p[23], \\\n                       CarrierDelay=int(p[24].replace(\"NA\", \"0\")), \\\n                                              CarrierDelayStr=p[24], \\\n                       WeatherDelay=int(p[25].replace(\"NA\", \"0\")), \\\n                                              WeatherDelayStr=p[25], \\\n                       NASDelay=int(p[26].replace(\"NA\", \"0\")), \\\n                       SecurityDelay=int(p[27].replace(\"NA\", \"0\")), \\\n                       LateAircraftDelay=int(p[28].replace(\"NA\", \"0\")))) \\\n         .toDF()\n\nflights.write.mode(\"ignore\").parquet(full_path(\"processed_local/flights\"))\nsqlContext.registerDataFrameAsTable(flights, \"flights\")\nflights.limit(10).toPandas()[[\"ArrDelay\",\"CarrierDelay\",\"CarrierDelayStr\",\"WeatherDelay\",\"WeatherDelayStr\",\"Distance\"]]","dateUpdated":"2017-01-23T08:55:34+0000","config":{"colWidth":12,"editorMode":"ace/mode/scala","title":true,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1485161264997_-613765952","id":"20170116-194514_1558643741","dateCreated":"2017-01-23T08:47:44+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:1179","focus":true},{"text":"","dateUpdated":"2017-01-23T08:47:44+0000","config":{"colWidth":12,"editorMode":"ace/mode/scala","graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1485161264997_-613765952","id":"20170116-200314_1592643376","dateCreated":"2017-01-23T08:47:44+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:1180"}],"name":"Python 2 data preparation","id":"2C9C4XSJA","angularObjects":{"2C3B8E6M6:shared_process":[],"2C6RJRBD1:shared_process":[]},"config":{"looknfeel":"default"},"info":{}}